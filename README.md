# Machine Learning Applications in Semiconductor Defect Detection

This project explores machine learning techniques to classify failure types in semiconductor wafer maps.Wafer maps are images that represent semiconductor wafers, and defect detection is crucial for the manufacturing process. Using the public WM-811K dataset of semiconductor wafer maps, we explore Naive Bayes and Convolutional Neural Networks (CNNs) as methods for identifying patterns in defective semiconductors. Specifically, this project aims to develop a model that can classify defective wafers into one of eight failure type categories: “Scratch”, “Edge-Ring”, “Edge-Loc”, “Center”, “Random”, “Loc”, “Near-full” and “Donut”.

After realizing Naive Bayes is not a good classifier for all eight failure type categories, we explored a simple 3-layer CNN as an alternative classification method. Building upon previous 

The Jupyter notebook provided walks through every step of the analysis from data acquisition, preprocessing, model training, and evaluation. All code, explanations, utility functions, and results are contained within the notebook.

### Repository Directory
- `main.ipynb`: A comprehensive notebook that includes data loading, preprocessing, model training, evaluation and results visualization
- `README.md`: A brief overview of the project and repository structure
- `requirements.txt file`: A list of all necessary Python packages to run the project
- Results: Visualizations and evaluation metrics such as accuracy and loss plots generated by the notebook


## Installation
To run the project, first install the necessary dependencies using the `requirements.txt` file. You can install them uses `pip` or `conda` by running:
```bash
pip install -r requirements.txt
```

### Data Acquisition
The data for this project is accessible via KaggleHub. In your terminal, install kagglehub using `pip` or `conda` through the following command:
```bash
pip install kagglehub
```
After installing kagglehub, the following line of code in `main.ipynb` file will correctly identify the file path to the WM-811K dataset:
```bash
path = kagglehub.dataset_download("qingyi/wm811k-wafer-map")
```
### `main.ipynb` Walkthrough
**1. Data Preprocessing:** This section explains how to load, clean and preprocess the WM-811K dataset. Since the wafer maps do not have a consistent size, two preprocessing techniques were explored to standardize the wafer map sizes:
    -   Bilinear Interpolation: Resize wafer maps using bilinear interpolation to a standard size. This method stretches the wafer map while keeping features recognizable, creating slight distortions.
    -   Padding: Pad the original wafer maps with zeros to fit a standard size, while keeping the original aspect ratio intact. This approach retains the integrity of the data without losing critical features from the wafer map, but greatly increases the number of zero values in the wafer maps.

Both preprocessing methods were tested using Naive Bayes and Convolutional Neural Networks to determine which would better preserve relevant information for the model.

**2. Classification Method 1 – Naive Bayes**: We initially explored  Naive Bayes as a baseline method for classifying the wafer maps. This model assumes that the features in the data (i.e. wafer maps) are independent and follow a Gaussian distribution. Although these assumptions might not perfectly hold for the wafer maps, Naive Bayes performed very well in classifying some failure types and terribly in others. This method was explored to evaluate the data before exploring more complex methods.

**3. Classification Method 2 – Convolutional Neural Network (CNN)**: In this section, we aimed to optimize the number of filters hyperparameter on a 3-layer CNN model to improve classification accuracy. The goal was to explore how changing the number of filters in the convolutional layers could affect the model's performance in classifying failure types in the wafer maps.

The architecture of the CNN model uses three convolutional layers using Rectified Linear Unit (ReLU) activations followed by max-pooling. By adjusting the number of filters in each convolutional layer, we sought to determine how different numbers of filters impacted the model's ability to learn and extract relevant features from the wafer maps. 

how changing the number of filters on a simple 3-layer CNN model changed the two different CNN architectures to classify the wafer maps' failure types. It describes how changes in architecture combined with hyperparameter optimization could help improve model performance. The following architectures were explored:
    -  Basic CNN: A simple 3-layer CNN model using Rectified Linear Unit (ReLU) activations for all convolutional and dense layers. The three convolutional layers were designed to capture different levels of features in the wafer maps, such as edges and textures. The first layer had 32 filters, second had 64, and third had 128 filters to ... The ReLU activation was chosen for its ability to introduct non-linearity while being computationally efficient.
    - Hyperparameter Optimization: To innovate and improve the model's performance, hyperparameter optimization was applied. This included experimenting with various hyperparameters 
This section details the second classification technique explored for the preprocessed data. Two Convolutional Neural Network architectures were explored to identify how hyperparameter optimization could modify 

**4. Results**: The results of the model training and evaluation include each model's accuracy, confusion matrix, and classification report featuring precision, recall and F1-score metrics.

